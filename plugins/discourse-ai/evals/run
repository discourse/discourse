#!/usr/bin/env ruby
# frozen_string_literal: true

require_relative "lib/boot"
require_relative "lib/llm_repository"
require_relative "lib/eval"
require_relative "lib/prompts/prompt_evaluator"
require_relative "lib/prompts/single_test_runner"
require_relative "lib/features"
require_relative "lib/recorder"
require_relative "lib/workbench"
require_relative "lib/cli"

features_registry =
  DiscourseAi::Evals::Features.new(modules: DiscourseAi::Configuration::Module.all)

options = DiscourseAi::Evals::Cli.parse_options!(features_registry)

llm_repository = DiscourseAi::Evals::LlmRepository.new

if options.list_models
  llm_repository.print
  exit 0
end

if options.list_features
  features_registry.print
  exit 0
end

available_evals = DiscourseAi::Evals::Eval.available_cases

if options.list
  available_evals.each(&:print)
  exit 0
end

llms = llm_repository.choose(options.models)

if llms.empty?
  puts "Error: Unknown models '#{options.models}'"
  exit 1
end

selected_evals = available_evals
selected_evals =
  selected_evals.select do |eval_case|
    eval_case.feature == options.feature_key
  end if options.feature_key.present?
selected_evals =
  selected_evals.select do |eval_case|
    eval_case.id == options.eval_name
  end if options.eval_name.present?

if selected_evals.empty?
  if options.feature_key
    puts "Error: No evaluations registered for feature '#{options.feature_key}'"
  else
    puts "Error: Unknown evaluation '#{options.eval_name}'"
  end
  exit 1
end

default_judge = "gpt-4o"
judge_name = options.judge.presence || default_judge
judge_llm = nil
default_judge_error = nil

begin
  judge_llm = llm_repository.hydrate(judge_name)
rescue StandardError => e
  if options.judge.present?
    puts "Error: #{e.message}"
    exit 1
  else
    judge_llm = nil
    default_judge_error = e.message
  end
end

if selected_evals.any? { |eval_case| eval_case.judge.present? } && judge_llm.nil?
  message = "Error: Selected evaluations require a judge."
  if default_judge_error
    message += " Configure '#{default_judge}' or pass --judge with an LLM config name."
    message += "\n\n"
    message += default_judge_error
  else
    message += " Pass --judge with an LLM config name."
  end
  puts message
  exit 1
end

playground = DiscourseAi::Evals::Workbench.new(output: $stdout, judge_llm: judge_llm)

selected_evals.each do |eval_case|
  if judge_llm.present? && eval_case.judge.blank?
    puts "Notice: Eval '#{eval_case.id}' has no judge block. --judge is ignored for this eval."
  end

  playground.run(eval_case: eval_case, llms: llms)
end
